{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initializing the OpenAI Client\n",
    "\n",
    "This section of the code is responsible for setting up the environment and initializing the OpenAI client, which we will use to interact with OpenAI APIs such as Whisper and TTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "_ = load_dotenv(find_dotenv()) \n",
    "\n",
    "# Initialize the OpenAI client with the API key\n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY'],   # Retrieves API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Components:\n",
    "\n",
    "- **Environment Variables**: We use `dotenv` to load environment variables. This is a secure way to manage sensitive information like API keys. The `.env` file should contain your `OPENAI_API_KEY`.\n",
    "\n",
    "- **OpenAI Client Initialization**: We create an instance of the `OpenAI` class from the `openai` package, passing the API key from the environment variables. This client will be used to make requests to OpenAI services.\n",
    "\n",
    "> ðŸ’¡ **Tip:** Always keep your API keys secure. Never hardcode them into your scripts. Using environment variables as shown here is a best practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Function to Transcribe Audio to Text\n",
    "\n",
    "This function, `get_transcript`, takes the path of an audio file and uses OpenAI's Whisper model to transcribe the audio to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(file_path):\n",
    "    # Open the audio file in binary read mode\n",
    "    audio_file = open(file_path, \"rb\")\n",
    "    \n",
    "    # Use the OpenAI Whisper model to transcribe the audio\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",           # Specifies the Whisper model to use\n",
    "        file=audio_file,             # Passes the audio file to the API\n",
    "        response_format=\"text\"       # Requests the transcription in text format\n",
    "    )\n",
    "\n",
    "    # Return the transcription\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points:\n",
    "\n",
    "- **Opening the File**: The audio file is opened in binary read mode (`\"rb\"`), which is required for audio data processing.\n",
    "\n",
    "- **Transcription Request**: The `client.audio.transcriptions.create` method is used to send the audio file to OpenAI's Whisper API for transcription.\n",
    "\n",
    "- **Model Specification**: Here, `\"whisper-1\"` is specified as the model. Depending on your needs and OpenAI's offerings, you might use a different model version.\n",
    "\n",
    "- **Returning the Transcript**: The function returns the transcription result, which can then be used for further processing or displayed in the notebook.\n",
    "\n",
    "> ðŸ’¡ **Note:**  Ensure that the audio file format and content are compatible with the Whisper API's requirements for accurate transcription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creating an AI Tutor Conversation Chain with Langchain\n",
    "\n",
    "This section demonstrates how to create a conversation chain for an AI tutor using the Langchain library, integrated with OpenAI's GPT model. The AI tutor is designed to simulate a conversational English teaching experience, where it actively participates in dialogues with a human (student) and provides corrections and suggestions in a conversational manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AI09\\AppData\\Local\\Temp\\ipykernel_10828\\1258771000.py:12: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(\n",
      "C:\\Users\\AI09\\AppData\\Local\\Temp\\ipykernel_10828\\1258771000.py:18: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary classes from the langchain and langchain_openai libraries\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "#LDH from langchain_openai import OpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Initialize the OpenAI model with a specified temperature.\n",
    "# Temperature set to 0 for deterministic, consistent responses.\n",
    "llm = OpenAI(\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Create a conversation buffer memory to keep track of the conversation.\n",
    "# This includes prefixes to distinguish between the AI tutor and the human user.\n",
    "memory = ConversationBufferMemory(\n",
    "    ai_prefix=\"AI Tutor:\",\n",
    "    human_prefix=\"Human:\",\n",
    ")\n",
    "\n",
    "# Function to get and configure the conversation chain\n",
    "def get_chain():\n",
    "    # Define a template for the conversation prompt.\n",
    "    # This template sets the context for the conversation and instructions for the AI.\n",
    "    prompt_template = \"\"\"\n",
    "    The following is a friendly conversation between a human and an AI.\n",
    "    The AI a top-tier English tutor with years of experience.\n",
    "    The AI is talking to a student who wants to practice speaking English. \n",
    "    The AI is to help the student practice speaking English by having a conversation. \n",
    "\n",
    "    The AI should feel free to correct the student's grammar and pronunciation and/or suggest different words or phrases to use whenever the AI feels needed.\n",
    "    And when the AI corrects the student, the AI must start the sentence with \"it is better to put it this way\"\n",
    "    But even when you correct the student, try to make a conversation first, and then correct the student\n",
    "\n",
    "    Current conversation:\n",
    "    {history}\n",
    "    Human: {input}\n",
    "    AI Tutor:\"\"\"\n",
    "\n",
    "    # Create a PromptTemplate object with the defined prompt template.\n",
    "    # This template includes variables for the conversation history and the latest human input.\n",
    "    conversation_prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=prompt_template)\n",
    "\n",
    "    # Initialize the conversation chain.\n",
    "    # This chain uses the defined prompt, the language model (llm), and the conversation memory.\n",
    "    conversation_chain = ConversationChain(\n",
    "        prompt=conversation_prompt,\n",
    "        llm=llm,\n",
    "        verbose=True,\n",
    "        memory=memory,\n",
    "    )\n",
    "    \n",
    "    # Return the configured conversation chain.\n",
    "    return conversation_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Key Components:\n",
    "\n",
    "- **Library Imports**: The script begins by importing necessary classes from `langchain` and `langchain_openai`, which are essential for setting up the conversation chain and integrating with OpenAI's model.\n",
    "\n",
    "- **OpenAI Model Initialization**: The `OpenAI` class is instantiated with a specified `temperature` parameter. A temperature of 0 is chosen for deterministic, consistent responses, making it suitable for an educational context where predictable, accurate outputs are preferred.\n",
    "\n",
    "- **Conversation Memory Setup**: A `ConversationBufferMemory` instance is created, defining prefixes to differentiate between the AI tutor's and the human's dialogues. This memory buffer helps in maintaining the context and flow of the conversation.\n",
    "\n",
    "- **Conversation Chain Function**: The `get_chain` function is defined to configure and return the conversation chain. Within this function:\n",
    "\n",
    "    - A `prompt_template` is defined, setting the context for the AI tutor's role and guidelines for the conversation.\n",
    "    - A `PromptTemplate` object is created using the defined template. This object facilitates the incorporation of conversation history and the latest input into the AI's response generation.\n",
    "    - Finally, a `ConversationChain` is instantiated, linking the prompt template, the language model (`llm`), and the conversation memory (`memory`). The `verbose` parameter is set to True for detailed output, useful for debugging and understanding the AI's decision-making process. \n",
    "        - Feel free to set `verbose` to `False` if you do not need to see how `Langchain` prompts its converstaion with `ChatGPT`\n",
    "\n",
    "\n",
    "> **ðŸ’¡ Tip:** This function plays a key role in maintaining the flow of conversation, ensuring that the AI's responses are contextually relevant and pedagogically sound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generating AI Responses for Conversational Interaction\n",
    "\n",
    "This section is focused on generating responses from an AI tutor in a conversational context. It leverages the conversation chain established earlier to process input transcripts and produce relevant, context-aware responses. This functionality is central to creating an interactive, AI-driven conversation experience, such as in language learning applications or chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_response(transcript):\n",
    "    # Talk to the AI Tutor via langchain \n",
    "    conversation = get_chain()\n",
    "    answer = conversation.predict(input=transcript)\n",
    "    \n",
    "    # Return the AI's message content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points:\n",
    "- **Function Definition**: The `get_gpt_response` function is defined to handle the processing of input transcripts and generate responses using the AI tutor.\n",
    "\n",
    "- **Conversation Chain Retrieval**: Inside the function, `get_chain()` is called to retrieve the pre-configured conversation chain. This chain is set up to utilize the Langchain library with a specific conversation context and rules, as defined in the previous code block.\n",
    "\n",
    "- **Generating the AI Response**: The predict method of the conversation chain is used to generate a response from the AI based on the given transcript (input text). This method considers the conversation's history and the AI's role as an English tutor, ensuring responses are contextually relevant and pedagogically sound.\n",
    "\n",
    "- **Return Statement**: The function returns the generated answer, allowing it to be used elsewhere in the application, such as in a user interface for displaying the AI's responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Function to Play AI Tutor's Response Using Text-to-Speech\n",
    "\n",
    "This function, `play_gpt_response_with_tts`, converts the AI tutor's textual response into speech using Text-to-Speech (TTS) and plays it aloud for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from playsound import playsound\n",
    "#LDH_added\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import subprocess\n",
    "\n",
    "# Path to temporarily store the generated speech file\n",
    "speech_file_path = f\"./speech_llm.wav\"\n",
    "fixed_speech_file_path = f\"./speech_llm_converted.wav\"    #LDH added\n",
    "\n",
    "def play_gpt_response_with_tts(gpt_response):\n",
    "    # Generate speech from the GPT response using TTS\n",
    "\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"tts-1\",          # Specifies the TTS model to use\n",
    "        voice=\"alloy\",          # Chooses a specific voice for the TTS\n",
    "        input=gpt_response      # The text input to be converted to speech\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Stream the audio to a file\n",
    "        response.stream_to_file(speech_file_path)\n",
    "    except AttributeError as e:\n",
    "        print(\"Wrong method:\", e)\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"Wrong file path:\", e)\n",
    "    except Exception as e:\n",
    "        print(\"Other errors:\", e)\n",
    "\n",
    "    #LDH Convert the LLM speech to the one which can be played by a ffmpeg command using subprocess\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            ['ffmpeg', '-i', speech_file_path, fixed_speech_file_path], \n",
    "            check=True,  # This will raise an error if ffmpeg fails\n",
    "            stdout=subprocess.PIPE,  # Capture stdout\n",
    "            stderr=subprocess.PIPE  # Capture stderr\n",
    "        )\n",
    "        print(f\"Conversion successful: {fixed_speech_file_path}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred: {e.stderr.decode('utf-8')}\")\n",
    "\n",
    "    # Play the generated speech audio\n",
    "    #LDH playsound(speech_file_path)\n",
    "    audio = AudioSegment.from_wav(fixed_speech_file_path)\n",
    "    play(audio)\n",
    "\n",
    "    # Remove the temporary speech file to clean up\n",
    "    os.remove(fixed_speech_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points:\n",
    "\n",
    "- **TTS Conversion**: The `client.audio.speech.create` method from the OpenAI API is used to convert the AI's textual response into speech. The `tts-1` model and `alloy` voice are specified here, but these can be adjusted based on your preferences.\n",
    "\n",
    "- **Temporary Audio File Handling**: The generated speech is streamed to a file named `speech.wav` stored at the given file path. This approach is used to handle the audio output efficiently.\n",
    "\n",
    "- **Audio Playback**: The `playsound` library plays the audio file, allowing the user to hear the AI's response.\n",
    "\n",
    "- **Cleanup**: After playing the audio, the temporary file is removed to avoid clutter and manage storage efficiently.\n",
    "\n",
    "> **ðŸ’¡ Note:** This function bridges the gap between textual AI responses and auditory output, making the interaction more engaging and accessible, especially for auditory learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Facilitating Dialogue with the AI Tutor Using Speech-to-Text and Text-to-Speech\n",
    "\n",
    "The function `talk_to_gpt` orchestrates the process of converting user speech to text, obtaining a response from the AI tutor, and then converting this response back to speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_to_gpt(file_path):\n",
    "    # Transcribe user speech to text\n",
    "    user_transcript = get_transcript(file_path)\n",
    "\n",
    "    # Get the GPT tutor's response to the user's transcript\n",
    "    # Uses only the last 10 messages in history for context\n",
    "    gpt_response = get_gpt_response(user_transcript)\n",
    "    \n",
    "    # Play the GPT response using OpenAI's TTS API\n",
    "    play_gpt_response_with_tts(gpt_response=gpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Components:\n",
    "\n",
    "- **Function Definition**: The `talk_to_gpt` function is designed to handle the complete cycle of user interaction with the AI tutor, from speech input to spoken response.\n",
    "\n",
    "- **Speech-to-Text Conversion**: The function begins by transcribing user speech into text. The `get_transcript` function is called with the `file_path` of the user's speech recording, converting the speech into a text transcript.\n",
    "\n",
    "- **AI Tutor Response Generation**: The text transcript is then passed to the `get_gpt_response` function. This function, as defined earlier, generates a context-aware response from the AI tutor, considering the ongoing conversation.\n",
    "\n",
    "- **Text-to-Speech Conversion**: The AI's textual response is then converted back into speech. The `play_gpt_response_with_tts` function takes the AI's response and uses OpenAI's Text-to-Speech (TTS) API to play the response. This creates an auditory output that the user can listen to.\n",
    "\n",
    "- **End-to-End Interaction**: By combining these steps, the function enables a fluid, conversational interaction between the user and the AI tutor. It transforms user speech into text, processes it through the AI, and returns a spoken response, thus completing the dialogue loop.\n",
    "\n",
    "> **ðŸ’¡ Note:** This function is central to the user interaction, seamlessly integrating speech-to-text, AI response generation, and text-to-speech to simulate a natural conversation flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Audio Recording Class for User Input\n",
    "\n",
    "The `AudioRecorder` class encapsulates the functionality needed to record audio from the user, which can then be processed for speech-to-text conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "\n",
    "class AudioRecorder:\n",
    "    def __init__(self):\n",
    "        self.is_recording = False      # Flag to control recording state\n",
    "        self.audio_data = []           # List to store audio frames\n",
    "        self.fs = 44100                # Sample rate (in Hz)\n",
    "        self.channels = 1              # Number of audio channels\n",
    "\n",
    "    def start_recording(self):\n",
    "        self.is_recording = True\n",
    "        self.audio_data = []\n",
    "        # Start recording in a separate thread\n",
    "        threading.Thread(target=self.record).start()\n",
    "\n",
    "    def stop_recording(self):\n",
    "        self.is_recording = False      # Stop the recording\n",
    "\n",
    "    def record(self):\n",
    "        # Set up the audio input stream\n",
    "        with sd.InputStream(samplerate=self.fs, channels=self.channels) as stream:\n",
    "            while self.is_recording:\n",
    "                data, _ = stream.read(1024)  # Read audio data from the input stream\n",
    "                self.audio_data.append(data)  # Append data to the audio_data list\n",
    "\n",
    "    def save(self, filename='speech_human.wav'):\n",
    "        # Save the recorded audio to a file\n",
    "        if self.audio_data:\n",
    "            wav_data = np.concatenate(self.audio_data, axis=0)  # Concatenate all audio frames\n",
    "            wavio.write(filename, wav_data, self.fs, sampwidth=2)  # Write to WAV file\n",
    "            print(\"Recording saved to\", filename)\n",
    "            return filename\n",
    "        else:\n",
    "            print(\"No recording data to save.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features:\n",
    "\n",
    "- **Initialization**: Sets up initial variables like sample rate, channels, and recording state.\n",
    "\n",
    "- **Start and Stop Recording**: Methods to control the start and stop of audio recording.\n",
    "\n",
    "- **Multithreading for Recording**: Uses a separate thread to handle audio input, ensuring the main program remains responsive.\n",
    "\n",
    "- **Audio Data Collection**: Continuously reads audio data from the microphone and stores it in a list.\n",
    "\n",
    "- **Saving the Recording**: Concatenates the recorded audio frames and saves them as a WAV file. This file can then be used for further processing like speech-to-text.\n",
    "\n",
    "> **ðŸ’¡ Note:** This class provides a foundational audio input mechanism, crucial for capturing the user's speech in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Interactive Interface for Audio Recording and Processing\n",
    "\n",
    "This section of the code creates an interactive interface using IPython widgets to control the audio recording and initiate conversation with the AI tutor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571b1877c9f149909bd226bc65f31ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Start Recording', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fc58ad2eef45399089b1b1c5e6ae1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Stop Recording', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Initialize the audio recorder\n",
    "recorder = AudioRecorder()\n",
    "\n",
    "# Create buttons for starting and stopping the recording\n",
    "start_button = widgets.Button(description=\"Start Recording\")\n",
    "stop_button = widgets.Button(description=\"Stop Recording\")\n",
    "\n",
    "def on_start_clicked(b):\n",
    "    # Function to handle start button click\n",
    "    recorder.start_recording()  # Start recording audio\n",
    "    print(\"Recording started...\")\n",
    "\n",
    "def on_stop_clicked(b):\n",
    "    # Function to handle stop button click\n",
    "    print(\"Recording stopped and saved.\")\n",
    "    recorder.stop_recording()  # Stop recording audio\n",
    "    file_name = recorder.save()  # Save the recorded audio to a file\n",
    "    talk_to_gpt(file_name)  # Process the audio file through the AI tutor\n",
    "    #LDH os.remove(file_name)  # Remove the temporary audio file\n",
    "\n",
    "# Assign the click event handlers to the buttons\n",
    "start_button.on_click(on_start_clicked)\n",
    "stop_button.on_click(on_stop_clicked)\n",
    "\n",
    "# Display the buttons in the Jupyter Notebook interface\n",
    "display(start_button, stop_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Components:\n",
    "\n",
    "- **Button Widgets**: Two buttons are created using `ipywidgets` for starting and stopping the audio recording.\n",
    "\n",
    "- **Event Handlers**: Functions `on_start_clicked` and `on_stop_clicked` are defined to handle the respective button clicks.\n",
    "    - `on_start_clicked` starts the audio recording.\n",
    "    - `on_stop_clicked` stops the recording, saves the audio, processes it through the AI tutor (`talk_to_gpt`), and then cleans up the temporary file.\n",
    "    \n",
    "- **Display Widgets**: The `display` function from `IPython.display` is used to render the buttons in the Jupyter Notebook.\n",
    "\n",
    "> **ðŸ’¡ Note:** This interactive setup allows users to easily control the recording process and seamlessly initiate interaction with the AI tutor, enhancing the user experience in the Jupyter Notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
